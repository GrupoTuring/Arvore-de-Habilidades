{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PCA.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"NYfWvHAd3HEQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7l_am48nQ3vl","colab_type":"text"},"source":["# PCA - Conceitos Básicos\n","Hoje vamos conhecer melhor essa ferramenta famosa e poderosa. Para iniciar, vamos começar com a lógica da sua implementação. Ele tenta reduzir os dados de forma que a nova representação possua a mesma variância. Explicando assim é difícil de entender, então vamos ao código!"]},{"cell_type":"code","metadata":{"id":"DzxOuK-URbrF","colab_type":"code","outputId":"ffb72828-e073-45b1-83db-871c42242e8a","executionInfo":{"status":"ok","timestamp":1580766570847,"user_tz":180,"elapsed":3203,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# A primeira parte é entender o que é variância. Variância é uma medida de \n","# disperção dos dados. Enquanto que a média e mediana indicam onde os dados\n","# se encontram, a variância e o desvio padrão indicam a sua disperção.\n","\n","# Vamos para exemplos:\n","\n","array_1 = np.array([5,5.5,4.5,6,4,5])\n","array_2 = np.array([0,0,0,10,10,10])\n","\n","# Vamos calcular os indicadores citados anteriormente\n","mean_1, median_1, var_1, std_1 = np.mean(array_1), np.median(array_1), np.var(array_1), np.std(array_1)\n","mean_2, median_2, var_2, std_2 = np.mean(array_2), np.median(array_2), np.var(array_2), np.std(array_2)\n","\n","# Mostra os resultados\n","print(mean_1, mean_2)\n","print(median_1, median_2)\n","print(var_1, var_2)\n","print(std_1, std_2)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["5.0 5.0\n","5.0 5.0\n","0.4166666666666667 25.0\n","0.6454972243679028 5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eeJLhE4kStWN","colab_type":"text"},"source":["Podemos ver que, embora essas duas listas possuam médias e medianas iguais, seu desvio padrão (std) e sua variância são totalmente diferentes. Isso faz sentido, visto que a primeira lista possui valores próximos, enquanto que a segunda possui valores bem distantes.\n","\n","Agora que sabemos o que é a variância de um conjunto de dados, vamos entender o que é a variância entre duas variáveis."]},{"cell_type":"code","metadata":{"id":"T2ftuvWxSss8","colab_type":"code","outputId":"1b8e277a-2146-49df-9a18-a2dc2ab9426a","executionInfo":{"status":"ok","timestamp":1580766570849,"user_tz":180,"elapsed":3191,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Covariância (variância entre duas variáveis) é uma medida que mostra\n","# a dependência de uma variável com outra. Covariância igual a 0 indica\n","# que as variáveis são DESCORRELACIONADAS (não confundir com independente,\n","# explico depois).\n","\n","def cov(data_1, data_2):\n","    \"\"\"\n","    Função que calcula covariância entre dois valores. \n","\n","    Parameters\n","    ----------\n","    data_1 : numpy.array\n","        Conjunto de dados.\n","    data_2 : numpy.array\n","        Segundo connjunto de dados.\n","\n","\n","    Returns\n","    -------\n","    covariânce : float\n","        Valor da covariância.\n","    \"\"\"\n","\n","    mean_1, mean_2 = data_1.mean(), data_2.mean() # calculando a média\n","    n = len(data_1)\n","    return ((data_1 - mean_1) * (data_2 - mean_2)).sum()/(n-1)\n","\n","print(cov(array_1, array_2))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7KPzPSmFobQy","colab_type":"text"},"source":["Nesses nossos dados, temos que nossos dados não possuem dependência direta. Ou seja, são descorrelacionadas. Mas por que eu não posso inferir que elas são independentes? \n","\n","Existe uma diferença entre ser descorrelacionada e independente. Duas variáveis X e Y são descorrelacionadas quando a Cov(X,Y) é igual a 0 e são independentes quando a Cov( f(X), g(Y) ) é igual a 0 para qualquer função f e g. Será que nossos pontos são independentes?"]},{"cell_type":"code","metadata":{"id":"FN2-IJUdSlyP","colab_type":"code","outputId":"4ff0c72e-fd59-43ef-811f-69238cbd5fd3","executionInfo":{"status":"ok","timestamp":1580766570850,"user_tz":180,"elapsed":3179,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Faremos 3 testes com eles\n","\n","print(cov(array_1, array_2**3))               # Aqui, a cov continua zero! \n","print(cov(np.log(array_1), array_2))          # Aqui, a cov é -0.03....\n","print(cov(np.exp(array_1), np.exp(array_2)))  # Aqui, a cov é 2.71e5....\n","\n","# Logo, eles não são independentes"],"execution_count":4,"outputs":[{"output_type":"stream","text":["0.0\n","-0.03077165866675391\n","271613.37469721073\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wvhoVD1mraNB","colab_type":"text"},"source":["Para o próximo passo, vamos falar sobre a matriz de covariância. Ela é o centro do algoritmo de PCA, então é de extrema importância falar dela."]},{"cell_type":"code","metadata":{"id":"Knf0uYJ-rZlo","colab_type":"code","outputId":"d08b3c89-1fdf-49aa-9c7c-6748e46ea456","executionInfo":{"status":"ok","timestamp":1580766570851,"user_tz":180,"elapsed":3166,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Essa matriz é uma matriz quadrada que mostra todas as covariâncias entre N  \n","# variáveis aleatórias. Na sua diagonal principal, há a covariância de cada\n","# variável com ela mesma. Vamos fazer um experimento com os nossos pontos.\n","\n","# Covariância entre o array_1 com ele mesmo.\n","cov_11 = cov(array_1, array_1)\n","\n","# Covariância entre o array_2 com ele mesmo.\n","cov_22 = cov(array_2, array_2)\n","\n","# Covariância entre o array_1 e o array_2.\n","cov_12 = cov(array_1, array_2)\n","\n","print(cov_11, cov_22, cov_12)\n","\n","# Vamos calcular a matriz de covariância dessas duas variáveis usando a função\n","# do numpy: np.cov\n","\n","matriz = np.cov(array_1, array_2)\n","print(matriz)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0.5 30.0 0.0\n","[[ 0.5  0. ]\n"," [ 0.  30. ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x10Lgd4ruAv3","colab_type":"text"},"source":["Temos então uma matriz 2x2 por estarmos usando duas variáveis aleatórias. O elemento Aij (lembre-se, i -> linha, j -> coluna) da matriz é o valor da covariância entre a variável i com a variável j. No nosso exemplo, A11 é igual a covariância da variável 1 com a 1, A12 = A21 é a covariância entre 1 com 2 e A22 é a covariância entre 2 e 2.\n","\n","Agora, por que essa matriz é importante pro PCA? Porque o seu objetivo é criar variáveis descorrelacionadas. Vamos entender mais de perto."]},{"cell_type":"code","metadata":{"id":"ehSZe-xOuAGZ","colab_type":"code","outputId":"17d160d7-4372-4e16-954c-591ad9389f73","executionInfo":{"status":"ok","timestamp":1580766570852,"user_tz":180,"elapsed":3155,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["# Vamos criar um conjunto de dados aleatórios, ver sua matriz de covariância,\n","# aplicar o PCA do Sklearn e calcular a matriz de covariância nos novos dados.\n","\n","np.random.seed(100) \n","\n","dados = np.random.random((100,3))*10 # Cria 100 pontos de 3 dimensões\n","\n","print(\"Cov dos dados originais:\")\n","print(np.cov(dados.T)) # O .T é para ele entender que possui 3 dimensões, não 100\n","\n","from sklearn.decomposition import PCA # PCA do Sklearn\n","\n","dados_pca = PCA().fit_transform(dados) # Transforma nossos dados\n","\n","print(\"\\n\\nCov dos dados pós pca:\")\n","print(np.cov(dados_pca.T))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Cov dos dados originais:\n","[[ 8.224208    0.43011632 -1.58685266]\n"," [ 0.43011632  8.62601916  0.30802666]\n"," [-1.58685266  0.30802666  8.41397601]]\n","\n","\n","Cov dos dados pós pca:\n","[[ 9.91294324e+00 -1.69262803e-16 -1.65954591e-15]\n"," [-1.69262803e-16  8.75723187e+00  1.97308034e-15]\n"," [-1.65954591e-15  1.97308034e-15  6.59402806e+00]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CS0_f8DlzVAd","colab_type":"text"},"source":["Vamos entender o resultado. Inicialmente, nossos dados não eram descorrelacionados, pois os valores fora da diagonal principal na matriz de correlação eram diferente de 0. Depois de aplicar o PCA, os valores fora da diagonal principal são muito próximos de 0. Então o PCA está transformando nossos dados em dados descorrelacionados.\n","\n","Por que criar dados descorrelacionados? Pois assim a covariância fica somente na diagonal principal da matriz, e conseguimos ver quais variáveis possuem mais covariância e qual possui menos. "]},{"cell_type":"markdown","metadata":{"id":"tjEllfiFz4Sl","colab_type":"text"},"source":["# PCA - Código\n","Então o objetivo do PCA é criar um novo conjunto de dados descorrelacionado a partir de um conjunto de dados existente. Em outras palavras, ele quer transformar uma matriz em uma matriz com valores apenas na diagonal, ou seja, ele quer **diagonalizar** a matriz. Esse problema é conhecido em algebra linear, e sua explicação está nesse [link](https://pt.wikipedia.org/wiki/Matriz_diagonaliz%C3%A1vel). \n","\n","Vamos codar!"]},{"cell_type":"code","metadata":{"id":"0RkFOFh9z3GL","colab_type":"code","outputId":"b5fd88eb-21fa-4966-ee3d-b7ed025537ad","executionInfo":{"status":"ok","timestamp":1580766570855,"user_tz":180,"elapsed":3128,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Para diagonalizar uma matriz, precisamos calcular os autovetores e  \n","# os autovalores dela. Utilizaremos a funçao do numpy: numpy.linalg.eig.\n","\n","cova_matriz = np.cov(dados.T)\n","\n","val, vec = np.linalg.eig(cova_matriz)\n","print(\"Os autovalores são:\")\n","print(val)\n","print(\"Os respectivos autovetores são:\")\n","print(vec)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Os autovalores são:\n","[6.59402806 9.91294324 8.75723187]\n","Os respectivos autovetores são:\n","[[-0.70843442  0.69150895  0.14119506]\n"," [ 0.25000645  0.05879045  0.96645769]\n"," [-0.66001322 -0.71997157  0.21453088]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ssn6cCNO2bUz","colab_type":"text"},"source":["Temos nossos autovalores (que indicam a covariância dos novos **componentes**) e nossos autovetores (que são nossos novos **componentes**). Componentes são os novos eixos que vamos criar com o PCA.\n","\n","**Isso é de extrema importância**: o PCA transforma os seus dados. O primeiro eixo dos dados originais não tem o mesmo significado que o primeiro componente do PCA, você perde **interpretabilidade** dos dados quando aplica PCA.\n","\n","Vamos continuar com o algoritmo."]},{"cell_type":"code","metadata":{"id":"_VYXDHeD2a6S","colab_type":"code","outputId":"9bb34cfd-e307-4f7f-9169-9d64b92801b5","executionInfo":{"status":"ok","timestamp":1580766570856,"user_tz":180,"elapsed":3114,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["# Agora que temos os autovetores, vamos tentar criar os dados que o PCA do \n","# Sklearn criou. Primeiramente, ordenamos os autovetores baseado no  seu \n","# respectivo autovalor.\n","\n","\n","args = np.argsort(-val) # Essa função retorna os indices do menor para o maior,\n","                        # como queremos o inverso, multiplicamos por -1.\n","\n","val_sort = val[args]   # Ordena os autovalores\n","vec_sort = vec[:,args] # Ordena os autovetores\n","\n","\n","dados_mean = dados - dados.mean(axis=0) # Vamos centralizar os dados\n","\n","dados_novos = dados_mean.dot(vec_sort) # Vamos calcular nossos novos componentes\n","\n","print(\"Nossos dados:\")\n","print(dados_novos[:3,:])\n","\n","print(\"\\n\\nDados PCA Sklearn:\")\n","print(dados_pca[:3,:])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Nossos dados:\n","[[ 0.09252165 -2.22066347 -0.24436895]\n"," [ 4.19679286 -5.08977681 -1.06403023]\n"," [ 3.36714311  2.63289181  2.12181569]]\n","\n","\n","Dados PCA Sklearn:\n","[[-0.09252165  2.22066347  0.24436895]\n"," [-4.19679286  5.08977681  1.06403023]\n"," [-3.36714311 -2.63289181 -2.12181569]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9cInwesBBuxp","colab_type":"text"},"source":["Podemos ver que há diferença entre o sinal, por causa que o PCA do Sklearn utiliza outra forma (SVD) para encontrar os componentes. Isso não influenciar o resultado, visto que **todos** os pontos só possuem o sinal diferente, o seu valor é o mesmo (visualmente, eles só estariam rotacionados 180º, a informação aprendida é a mesma).\n","\n","Mas ainda não houve a redução, só mudança de base. Vamos para a parte final do algoritmo."]},{"cell_type":"code","metadata":{"id":"zXbqu9_yCfrn","colab_type":"code","outputId":"0546d5c3-6c06-4351-e660-b6c349c22e7a","executionInfo":{"status":"ok","timestamp":1580766570857,"user_tz":180,"elapsed":3100,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["# Para reduzir a dimensionalidade, temos que definir quanto de informação\n","# queremos manter. O que isso significa? Cada componente possui informação\n","# do problema, e queremos manter aqueles que possuem mais informação.\n","# A informação, no algoritmo do PCA, é a covariância dos dados, pois\n","# ele entende que variaveis com pouca variância trazem pouca informação, \n","# mas aquelas que variam muito são importantes. \n","#\n","# Visto isso, a informação está relacionada com os autovalores\n","\n","print(\"Nossos autovalores:\")\n","print(val_sort)\n","\n","info_total = val_sort.sum() # Aqui calculamos a informação total\n","\n","print(f\"\\n\\nInformação total: {info_total}\") \n","\n","print(\"\\n\\nInformação de cada autovalor:\")\n","print(val_sort/info_total)\n","\n","print(\"\\n\\nInformação pelo Sklearn:\")\n","print(PCA().fit(dados).explained_variance_ratio_) # É igual ao nosso"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Nossos autovalores:\n","[9.91294324 8.75723187 6.59402806]\n","\n","\n","Informação total: 25.26420316222461\n","\n","\n","Informação de cada autovalor:\n","[0.3923711  0.34662609 0.26100281]\n","\n","\n","Informação pelo Sklearn:\n","[0.3923711  0.34662609 0.26100281]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SWbUZD75EA5-","colab_type":"text"},"source":["A ultima parte do algoritmo é decidirmos o quanto de informação queremos. Por exemplo, se quisessemos manter 70% da informação no nosso problema, o ultimo componente seria excluido, pois os 2 primeiros já garantem 70%. Valores normais na literatura são 95% e 99%.\n","\n","Vamos fazer o mesmo processo com uma reta."]},{"cell_type":"code","metadata":{"id":"_i8XeMEbGJy-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"801b2749-aa22-4880-da4e-b681abac5ec2","executionInfo":{"status":"ok","timestamp":1580766590707,"user_tz":180,"elapsed":1491,"user":{"displayName":"Felipe Augusto de Moraes Machado","photoUrl":"","userId":"07018346585176710755"}}},"source":["np.random.seed(100)\n","\n","x = np.random.random((100,1))\n","y = 2*x + 0.1*np.random.random((100,1))\n","\n","dados_reta = np.concatenate([x,y], axis=1)\n","cova_reta = np.cov(dados_reta.T)\n","val_reta, vec_reta = np.linalg.eig(cova_reta)\n","\n","args_reta = np.argsort(-val_reta) \n","\n","val_reta_sort = val_reta[args_reta]   \n","vec_reta_sort = vec_reta[:,args_reta] \n","\n","dados_reta_mean = dados_reta - dados_reta.mean(axis=0) \n","dados_novos_reta = dados_reta_mean.dot(vec_reta_sort) \n","\n","info_total_reta = val_reta_sort.sum()\n","print(\"Informação de cada autovalor:\")\n","print(val_reta_sort/info_total_reta)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Informação de cada autovalor:\n","[9.99604608e-01 3.95391716e-04]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4K3ZLovXCBbw","colab_type":"text"},"source":["Podemos ver que, para uma reta de 2 dimensões, o PCA consegue criar um componente com 99.6% de informação. Isso se dá pelo fato de que o PCA é um algoritmo **linear**! Mas isso não significa que ele seja ruim com dados não-lineares, pelo contrário, é muito comum utilizar ele nesses casos pois ele ainda é capaz de reduzir dimensão (mas não na mesma magnitude de um algoritmo não linear).\n","\n","Vamos colocar tudo junto e fazer uma função?"]},{"cell_type":"code","metadata":{"id":"vOuklXDGB3Ni","colab_type":"code","colab":{}},"source":["def PCATuring(dados, nova_dimensao):\n","    \"\"\"\n","    Função de PCA.\n","\n","    Parameters\n","    ----------\n","    dados : np.array\n","        Dados para aplicação do PCA. Dimensão NxD, onde N é \n","        o número de pontos e D a dimensão dos dados.\n","    nova_dimensao : int\n","        Valor para nova dimensão.\n","\n","    Returns\n","    -------\n","    novos_dados : np.array\n","        Dados criados pelo PCA.\n","    info : np.array\n","        Array com a informação de cada Componente.\n","\n","    \"\"\"\n","    cova = np.cov(dados.T)\n","    val, vec = np.linalg.eig(cova)\n","\n","    args = np.argsort(-val) \n","\n","    val_sort = val[args]   \n","    vec_sort = vec[:,args] \n","\n","    dados_mean = dados - dados.mean(axis=0) \n","    dados_novos = dados_mean.dot(vec_sort) \n","\n","    info = val_sort/val_sort.sum()\n","    return dados_novos[:,:nova_dimensao], info"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gPzheibHAGO","colab_type":"text"},"source":["Pronto! Acabamos o algoritmo. "]},{"cell_type":"markdown","metadata":{"id":"BNR5ihwNHZ9q","colab_type":"text"},"source":["# Comentários adicionais\n","O PCA, como foi dito anteriormente, é um algoritmo linear. Mesmo assim, ele é aplicado em diversos problemas, desde dados mais simples até sinais biológicos, como EEG. Essa sua fama é explicada por esse algoritmo não precisar de nenhum parâmetro, ele só depende dos dados do problema.\n","\n","Cabe ressaltar que, embora ele seja aplicável sempre, há alguns detalhes que devem ser considerados antes de aplicar. O primeiro é a **escala**. Não faz sentido comparar variância de uma variável de média 0.01 e std 0.005 com uma variável de média 100000 e std de 10000. Nesses casos, é bom **padronizar** os dados, ou seja, subtrair a média e dividir pelo desvio padrão. Isso torna a comparação mais justa. Um detalhe é que, quando se padroniza os dados, a matriz de covariância se torna a matriz de **correlação**."]}]}